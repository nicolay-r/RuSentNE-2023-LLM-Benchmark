# RuSentNE-LLM-Benchmark
[![arXiv](https://img.shields.io/badge/arXiv-2305.17679-b31b1b.svg)](https://arxiv.org/abs/2305.17679)


This repository assess the LLMs reasoning capabilities in Targeted Sentiment Analysis on [RuSentNE](https://arxiv.org/abs/2305.17679) dataset proposed as a part of the [self-titled competitions](https://github.com/dialogue-evaluation/RuSentNE-evaluation). 

In particular, the following **evaluation modes** were considered:
1. Zero-Shot-Learning
2. Fine-Tuning

On two RuSentNE dataset splits:
1. ðŸ”“ [Development](#development-results)
2. ðŸ”’ [Final](#final-results) [![arXiv](https://img.shields.io/badge/arXiv-2305.17679-b31b1b.svg)](https://arxiv.org/abs/2305.17679)

Using the following **Reasoning** approaches:
* Instruction prompts
* Chain-of-Thoughts

## ðŸ”“ Develpment Results


## ðŸ”’ Final Results
